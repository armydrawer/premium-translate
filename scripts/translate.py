#!/usr/bin/env python3
"""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å —Ä—É—Å—Å–∫–æ–≥–æ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π.
–û—Å–Ω–æ–≤–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
- –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ —Å –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ–º –ø–µ—Ä–µ–≤–æ–¥–∞
- –£–ª—É—á—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
- –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
- –£–ª—É—á—à–µ–Ω–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
"""

import os
import argparse
import json
import re
import sys
import time
import logging
from pathlib import Path
from typing import Dict, Optional, Tuple, List
from shutil import copyfile
import gc

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã
try:
    import torch
    from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
    HAS_TRANSFORMERS = True
except ImportError:
    HAS_TRANSFORMERS = False

class MarkdownPreserver:
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ Markdown —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self):
        self.placeholders = {}
        self.counter = 0
    
    def preserve(self, text: str) -> str:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ Markdown —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
        self.placeholders = {}
        self.counter = 0
        
        patterns = [
            (r'```[\s\S]*?```', 'CODE_BLOCK'),
            (r'`[^`\n]+?`', 'INLINE_CODE'),
            (r'!\[[^\]]*\]\([^)]+\)', 'IMAGE'),
            (r'\[[^\]]+\]\([^)]+\)', 'LINK'),
            (r'<!--[\s\S]*?-->', 'COMMENT'),
            (r'<[^>]+>', 'HTML_TAG'),
        ]
        
        for pattern, element_type in patterns:
            def replace_func(match):
                placeholder = f'__MD_{element_type}_{self.counter}__'
                self.placeholders[placeholder] = match.group(0)
                self.counter += 1
                return placeholder
            
            text = re.sub(pattern, replace_func, text, flags=re.MULTILINE)
        
        return text
    
    def restore(self, text: str) -> str:
        """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
        for placeholder, original in self.placeholders.items():
            text = text.replace(placeholder, original)
        return text

class AITranslator:
    """AI –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏"""
    
    def __init__(self, model_name: str = "Helsinki-NLP/opus-mt-ru-en"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        if self.model is not None:
            return
        
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–≤–æ–¥–∞: {self.model_name}")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name)
            self.model = self.model.to(self.device)
            logger.info(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise
    
    def translate(self, text: str, max_length: int = 512) -> str:
        """–ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞"""
        if not text.strip():
            return text
        
        self.load_model()
        
        try:
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=max_length
            ).to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_length,
                    num_beams=2,
                    early_stopping=True
                )
            
            translated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            return translated.strip()
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
            return text
    
    def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        if self.model:
            del self.model
            del self.tokenizer
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        gc.collect()

class DocumentTranslator:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.md_preserver = MarkdownPreserver()
        self.ai_translator = None
        self.stats = {'processed': 0, 'errors': 0}
    
    def setup_translators(self, use_ai: bool = True):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–æ–≤"""
        if use_ai and HAS_TRANSFORMERS:
            try:
                self.ai_translator = AITranslator()
                self.ai_translator.load_model()
            except Exception as e:
                logger.warning(f"AI –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                self.ai_translator = None
    
    def translate_text(self, text: str) -> str:
        """–ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º Markdown"""
        if not text.strip():
            return text
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º Markdown —ç–ª–µ–º–µ–Ω—Ç—ã
        preserved_text = self.md_preserver.preserve(text)
        
        # –ü–µ—Ä–µ–≤–æ–¥–∏–º
        if self.ai_translator:
            max_tokens = self.config.get('max_tokens', 512)
            translated = self.ai_translator.translate(preserved_text, max_tokens)
        else:
            translated = preserved_text  # –ü—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª, –µ—Å–ª–∏ AI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Markdown
        result = self.md_preserver.restore(translated)
        return result
    
    def process_markdown_file(self, src_path: str, dest_path: str) -> bool:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ Markdown —Ñ–∞–π–ª–∞"""
        try:
            with open(src_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            translated_content = self.translate_text(content)
            
            os.makedirs(os.path.dirname(dest_path), exist_ok=True)
            with open(dest_path, 'w', encoding='utf-8') as f:
                f.write(translated_content)
            
            logger.info(f"–ü–µ—Ä–µ–≤–µ–¥–µ–Ω: {Path(src_path).name}")
            self.stats['processed'] += 1
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {src_path}: {e}")
            self.stats['errors'] += 1
            return False
    
    def process_image_file(self, src_path: str, dest_path: str) -> bool:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            os.makedirs(os.path.dirname(dest_path), exist_ok=True)
            copyfile(src_path, dest_path)
            logger.info(f"–°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ: {Path(src_path).name}")
            self.stats['processed'] += 1
            return True
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è {src_path}: {e}")
            self.stats['errors'] += 1
            return False
    
    def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        if self.ai_translator:
            self.ai_translator.cleanup()

def load_config(config_path: str) -> Dict:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    default_config = {
        'max_files_per_run': 20,
        'max_tokens': 512,
        'max_file_size': 1048576,
    }
    
    if not config_path or not os.path.exists(config_path):
        return default_config
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return {**default_config, **json.load(f)}
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
        return default_config

def collect_files(source_dir: str, target_dir: str, max_files: int) -> List[Tuple[str, str]]:
    """–°–±–æ—Ä —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    files_to_process = []
    
    for root, dirs, files in os.walk(source_dir):
        dirs[:] = [d for d in dirs if not d.startswith('.')]
        
        for file in files:
            if file.startswith('.'):
                continue
                
            src_path = os.path.join(root, file)
            rel_path = os.path.relpath(src_path, source_dir)
            dest_path = os.path.join(target_dir, rel_path)
            
            if not os.path.exists(dest_path):
                files_to_process.append((src_path, dest_path))
            
            if len(files_to_process) >= max_files:
                break
        
        if len(files_to_process) >= max_files:
            break
    
    return files_to_process

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description='–ü–µ—Ä–µ–≤–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏')
    parser.add_argument('--source_dir', default='./ru', help='–ò—Å—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è')
    parser.add_argument('--target_dir', default='./en', help='–¶–µ–ª–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è')
    parser.add_argument('--config', help='–§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ JSON')
    parser.add_argument('--no-ai', action='store_true', help='–û—Ç–∫–ª—é—á–∏—Ç—å AI –ø–µ—Ä–µ–≤–æ–¥')
    parser.add_argument('--max-files', type=int, help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.source_dir):
        logger.error(f"–ò—Å—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {args.source_dir}")
        return False
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    config = load_config(args.config)
    if args.max_files:
        config['max_files_per_run'] = args.max_files
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞
    translator = DocumentTranslator(config)
    translator.setup_translators(use_ai=not args.no_ai)
    
    # –°–±–æ—Ä —Ñ–∞–π–ª–æ–≤
    max_files = config.get('max_files_per_run', 20)
    files_to_process = collect_files(args.source_dir, args.target_dir, max_files)
    
    if not files_to_process:
        logger.info("–ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return True
    
    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(files_to_process)} —Ñ–∞–π–ª–æ–≤")
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤
    start_time = time.time()
    
    try:
        for src_path, dest_path in files_to_process:
            if src_path.endswith('.md'):
                translator.process_markdown_file(src_path, dest_path)
            else:
                translator.process_image_file(src_path, dest_path)
    
    finally:
        translator.cleanup()
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    elapsed = time.time() - start_time
    stats = translator.stats
    
    logger.info("=" * 50)
    logger.info("–ü–ï–†–ï–í–û–î –ó–ê–í–ï–†–®–ï–ù")
    logger.info(f"‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['processed']}")
    logger.info(f"‚úó –û—à–∏–±–∫–∏: {stats['errors']}")
    logger.info(f"‚è± –í—Ä–µ–º—è: {elapsed:.1f}—Å")
    logger.info(f"üß† –†–µ–∂–∏–º AI: {'–í–ö–õ' if translator.ai_translator else '–í–´–ö–õ'}")
    logger.info("=" * 50)
    
    return stats['errors'] == 0

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        logger.info("–ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1)
