#!/usr/bin/env python3
"""
–û—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.
–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç .md —Ñ–∞–π–ª—ã –∏ .png –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
"""
import os
import argparse
import json
import re
import sys
from pathlib import Path
from typing import Dict, Optional, Tuple
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–π AI
try:
    import torch
    from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
    HAS_TRANSFORMERS = True
except ImportError:
    HAS_TRANSFORMERS = False
    logger.warning("Transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. AI –ø–µ—Ä–µ–≤–æ–¥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è OCR
try:
    import cv2
    import numpy as np
    from PIL import Image
    import pytesseract
    HAS_OCR = True
except ImportError:
    HAS_OCR = False
    logger.warning("OCR –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞.")

from shutil import copyfile

def load_glossary(glossary_path: Optional[str]) -> Dict[str, str]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –≥–ª–æ—Å—Å–∞—Ä–∏—è –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤"""
    if not glossary_path or not os.path.exists(glossary_path):
        return {}
    try:
        with open(glossary_path, 'r', encoding='utf-8') as f:
            glossary = json.load(f)
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω –≥–ª–æ—Å—Å–∞—Ä–∏–π —Å {len(glossary)} —Ç–µ—Ä–º–∏–Ω–∞–º–∏")
            return glossary
    except (json.JSONDecodeError, IOError) as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≥–ª–æ—Å—Å–∞—Ä–∏—è: {e}")
        return {}

def apply_glossary(text: str, glossary: Dict[str, str]) -> str:
    """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≥–ª–æ—Å—Å–∞—Ä–∏—è –∫ —Ç–µ–∫—Å—Ç—É"""
    if not glossary:
        return text
    for term, translation in glossary.items():
        pattern = r'\b' + re.escape(term) + r'\b'
        text = re.sub(pattern, translation, text, flags=re.IGNORECASE)
    return text

def preserve_markdown_elements(text: str) -> Tuple[str, Dict[str, str]]:
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Ä–∞–∑–º–µ—Ç–∫–∏ –æ—Ç –ø–µ—Ä–µ–≤–æ–¥–∞"""
    placeholders = {}
    counter = 0
    patterns = [
        (r'``````', 'CODE_BLOCK'), 
        (r'`[^`\n]+`', 'INLINE_CODE'), 
        (r'\[([^\]]*)\]\(([^)]*)\)', 'LINK'),
        (r'<!--[\s\S]*?-->', 'COMMENT'),
        (r'<[^>]+>', 'HTML_TAG'),
        (r'\!\[([^\]]*)\]\(([^)]*)\)', 'IMAGE'),
        (r'\{%[\s\S]*?%\}', 'TEMPLATE'),
        (r'\$\$[\s\S]*?\$\$', 'MATH_BLOCK'),
        (r'\$[^$\n]+\$', 'MATH_INLINE'),
    ]
    for pattern, element_type in patterns:
        def replace_func(match):
            nonlocal counter
            placeholder = f'__PLACEHOLDER_{element_type}_{counter}__'
            placeholders[placeholder] = match.group(0)
            counter += 1
            return placeholder
        text = re.sub(pattern, replace_func, text)
    return text, placeholders

def restore_preserved_elements(text: str, placeholders: Dict[str, str]) -> str:
    """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
    for placeholder, original in placeholders.items():
        text = text.replace(placeholder, original)
    return text

def translate_text_simple(text: str, glossary: Optional[Dict[str, str]] = None) -> str:
    """–ü—Ä–æ—Å—Ç–æ–π –ø–µ—Ä–µ–≤–æ–¥ –±–µ–∑ AI –º–æ–¥–µ–ª–∏ (–∑–∞–≥–ª—É—à–∫–∞)"""
    if not text.strip():
        return text
    if glossary:
        text = apply_glossary(text, glossary)
    logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ—Å—Ç–æ–π —Ä–µ–∂–∏–º –ø–µ—Ä–µ–≤–æ–¥–∞ (–∑–∞–≥–ª—É—à–∫–∞)")
    return text

def translate_text_ai(text: str, model, tokenizer, glossary: Optional[Dict[str, str]] = None, max_length: int = 512) -> str:
    """–ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º AI –º–æ–¥–µ–ª–∏"""
    if not text.strip():
        return text
    if glossary:
        text = apply_glossary(text, glossary)
    text_to_translate, placeholders = preserve_markdown_elements(text)
    if not text_to_translate.strip():
        return text
    try:
        chunks = []
        sentences = re.split(r'(?<=[.!?])\s+', text_to_translate)
        current_chunk = ""
        for sentence in sentences:
            if len(current_chunk + sentence) > max_length:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = sentence
                else:
                    chunks.append(sentence[:max_length])
            else:
                current_chunk += " " + sentence if current_chunk else sentence
        if current_chunk:
            chunks.append(current_chunk.strip())
        translated_chunks = []
        for chunk in chunks:
            if not chunk.strip():
                translated_chunks.append(chunk)
                continue
            inputs = tokenizer(
                chunk, 
                return_tensors="pt", 
                padding=True, 
                truncation=True,
                max_length=max_length
            ).to(model.device)
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_length,
                    num_beams=4,
                    early_stopping=True,
                    temperature=0.7
                )
            translated_chunk = tokenizer.decode(outputs[0], skip_special_tokens=True)
            translated_chunks.append(translated_chunk)
        translated_text = " ".join(translated_chunks)
        translated_text = restore_preserved_elements(translated_text, placeholders)
        return translated_text
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ AI –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
        return text

def process_markdown_file(src_path: str, dest_path: str, model, tokenizer, glossary: Dict[str, str]) -> bool:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ Markdown —Ñ–∞–π–ª–∞"""
    try:
        with open(src_path, 'r', encoding='utf-8') as f:
            content = f.read()
        os.makedirs(os.path.dirname(dest_path), exist_ok=True)
        if model and tokenizer:
            translated_content = translate_text_ai(content, model, tokenizer, glossary)
        else:
            translated_content = translate_text_simple(content, glossary)
        with open(dest_path, 'w', encoding='utf-8') as f:
            f.write(translated_content)
        logger.info(f"‚úì –ü–µ—Ä–µ–≤–µ–¥–µ–Ω: {src_path} -> {dest_path}")
        return True
    except Exception as e:
        logger.error(f"‚úó –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {src_path}: {e}")
        return False

def extract_text_from_image(image_path: str, lang: str = 'rus') -> Tuple[str, Optional['Image.Image']]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é Tesseract OCR"""
    if not HAS_OCR:
        logger.warning("OCR –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç")
        return "", None
    try:
        image = Image.open(image_path)
        text = pytesseract.image_to_string(image, lang=lang)
        return text.strip(), image
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ {image_path}: {e}")
        return "", None

def overlay_text_on_image(image: 'Image.Image', translated_text: str) -> Optional['Image.Image']:
    """–ù–∞–ª–æ–∂–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"""
    if not HAS_OCR:
        return image
    try:
        image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        height, width = image_cv.shape[:2]
        text_height = 100
        new_height = height + text_height
        new_image = np.ones((new_height, width, 3), dtype=np.uint8) * 255
        new_image[:height, :] = image_cv

        words = translated_text.split()
        lines = []
        current_line = ""
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.6
        thickness = 1
        max_width = width - 20

        for word in words:
            test_line = current_line + " " + word if current_line else word
            text_size = cv2.getTextSize(test_line, font, font_scale, thickness)[0]
            if text_size[0] <= max_width:
                current_line = test_line
            else:
                if current_line:
                    lines.append(current_line)
                    current_line = word
                else:
                    lines.append(word)
        if current_line:
            lines.append(current_line)

        line_height = 20
        start_y = height + 25
        for i, line in enumerate(lines[:3]):
            y = start_y + i * line_height
            cv2.putText(
                new_image,
                line,
                (10, y),
                font,
                font_scale,
                (0, 0, 0),
                thickness,
                cv2.LINE_AA
            )
        return Image.fromarray(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB))
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –Ω–∞–ª–æ–∂–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {e}")
        return image

def process_image_file(src_path: str, dest_path: str, model=None, tokenizer=None, glossary: Optional[Dict[str, str]] = None) -> bool:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    try:
        os.makedirs(os.path.dirname(dest_path), exist_ok=True)
        if not HAS_OCR:
            copyfile(src_path, dest_path)
            logger.info(f"‚úì –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {src_path} -> {dest_path}")
            return True
        original_text, image = extract_text_from_image(src_path)
        if not original_text or not image:
            copyfile(src_path, dest_path)
            logger.info(f"‚úì –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω): {src_path} -> {dest_path}")
            return True
        if model and tokenizer:
            translated_text = translate_text_ai(original_text, model, tokenizer, glossary)
        else:
            translated_text = translate_text_simple(original_text, glossary)
        new_image = overlay_text_on_image(image, translated_text)
        if new_image:
            new_image.save(dest_path)
            logger.info(f"‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å —Ç–µ–∫—Å—Ç–æ–º: {src_path} -> {dest_path}")
        else:
            copyfile(src_path, dest_path)
            logger.info(f"‚úì –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {src_path} -> {dest_path}")
        return True
    except Exception as e:
        logger.error(f"‚úó –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {src_path}: {e}")
        return False

def should_process_file(src_path: str, dest_path: str, force: bool = False) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞, –Ω—É–∂–Ω–æ –ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ñ–∞–π–ª"""
    if force:
        return True
    if not os.path.exists(dest_path):
        return True
    return os.path.getmtime(src_path) > os.path.getmtime(dest_path)

def load_translation_model():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–≤–æ–¥–∞"""
    if not HAS_TRANSFORMERS:
        logger.warning("Transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, AI –ø–µ—Ä–µ–≤–æ–¥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        return None, None
    model_name = "Helsinki-NLP/opus-mt-ru-en"
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model = model.to(device)
        logger.info(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {device.upper()}")
        return model, tokenizer
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return None, None

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description='Translate documentation from Russian to English')
    parser.add_argument('--source_dir', default='./ru', help='Source directory with Russian content')
    parser.add_argument('--target_dir', default='./en', help='Target directory for English content')
    parser.add_argument('--glossary', help='Path to glossary JSON file')
    parser.add_argument('--force', action='store_true', help='Force reprocessing of all files')
    parser.add_argument('--no-ai', action='store_true', help='Disable AI translation')
    parser.add_argument('--verbose', '-v', action='store_true', help='Verbose output')
    parser.add_argument('--max-chars', type=int, default=10000, help='Maximum characters per translation chunk')
    parser.add_argument('--max-tokens', type=int, default=512, help='Maximum tokens per model input')
    parser.add_argument('--max-file-size', type=int, default=1024*1024, help='Maximum file size in bytes (1MB default)')

    args = parser.parse_args()
    if args.verbose:
        logger.setLevel(logging.DEBUG)

    if not os.path.exists(args.source_dir):
        logger.error(f"–ò—Å—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {args.source_dir}")
        return False

    glossary = load_glossary(args.glossary)

    model, tokenizer = None, None
    if not args.no_ai:
        model, tokenizer = load_translation_model()
        if model and tokenizer:
            logger.info(f"–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–µ—Ä–µ–≤–æ–¥–∞: –º–∞–∫—Å. {args.max_chars} —Å–∏–º–≤–æ–ª–æ–≤, {args.max_tokens} —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –∫—É—Å–æ–∫")

    processed_count = 0
    error_count = 0
    skipped_count = 0
    oversized_count = 0

    # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –æ–±—Ö–æ–¥ –∏—Å—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    for root, dirs, files in os.walk(args.source_dir):
        for file in files:
            src_path = os.path.join(root, file)
            rel_path = os.path.relpath(src_path, args.source_dir)
            dest_path = os.path.join(args.target_dir, rel_path)
            if file.startswith('.') or file in ['Thumbs.db', '.DS_Store']:
                continue
            try:
                file_size = os.path.getsize(src_path)
                if file_size > args.max_file_size:
                    logger.warning(f"–§–∞–π–ª {src_path} –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç —Ä–∞–∑–º–µ—Ä–∞ ({file_size} > {args.max_file_size} –±–∞–π—Ç)")
                    oversized_count += 1
                    continue
            except OSError:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ {src_path}")
                error_count += 1
                continue

            if not should_process_file(src_path, dest_path, args.force):
                skipped_count += 1
                continue

            success = False
            if file.endswith('.md'):
                success = process_markdown_file(src_path, dest_path, model, tokenizer, glossary)
            elif file.endswith(('.png', '.jpg', '.jpeg')):
                success = process_image_file(src_path, dest_path, model, tokenizer, glossary)
            else:
                try:
                    os.makedirs(os.path.dirname(dest_path), exist_ok=True)
                    copyfile(src_path, dest_path)
                    logger.info(f"‚úì –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω: {src_path} -> {dest_path}")
                    success = True
                except Exception as e:
                    logger.error(f"‚úó –û—à–∏–±–∫–∞ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è {src_path}: {e}")
                    success = False
            if success:
                processed_count += 1
            else:
                error_count += 1

    logger.info("="*50)
    logger.info("–ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    logger.info(f"‚úì –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_count}")
    logger.info(f"‚úó –û—à–∏–±–æ–∫: {error_count}")
    logger.info(f"‚è≠ –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å): {skipped_count}")
    logger.info(f"üìè –ü—Ä–æ–ø—É—â–µ–Ω–æ (–ø—Ä–µ–≤—ã—à–µ–Ω —Ä–∞–∑–º–µ—Ä): {oversized_count}")
    logger.info(f"üìä –õ–∏–º–∏—Ç—ã: {args.max_chars} —Å–∏–º–≤–æ–ª–æ–≤, {args.max_tokens} —Ç–æ–∫–µ–Ω–æ–≤, {args.max_file_size//1024}KB —Ñ–∞–π–ª")
    logger.info("="*50)
    return error_count == 0

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        logger.info("–ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1)
