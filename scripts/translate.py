#!/usr/bin/env python3

"""
–û—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.
–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç .md —Ñ–∞–π–ª—ã –∏ .png –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ–∞–π–ª–æ–≤.
"""

import os
import argparse
import json
import re
import sys
import time
import gc
from pathlib import Path
from typing import Dict, Optional, Tuple, List
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ä–µ—Å—É—Ä—Å–æ–≤
try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False
    logger.warning("psutil –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–π AI
try:
    import torch
    from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
    HAS_TRANSFORMERS = True
except ImportError:
    HAS_TRANSFORMERS = False
    logger.warning("Transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. AI –ø–µ—Ä–µ–≤–æ–¥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è OCR
try:
    import cv2
    import numpy as np
    from PIL import Image
    import pytesseract
    HAS_OCR = True
except ImportError:
    HAS_OCR = False
    logger.warning("OCR –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞.")

from shutil import copyfile

class TranslationConfig:
    """–ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –ø–µ—Ä–µ–≤–æ–¥–∞"""
    
    def __init__(self, config_path: Optional[str] = None):
        self.config = self._load_default_config()
        if config_path and os.path.exists(config_path):
            self._load_config(config_path)
    
    def _load_default_config(self) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        return {
            "limits": {
                "max_files_per_run": 20,
                "max_chars_per_chunk": 10000,
                "max_tokens_per_input": 512,
                "max_file_size_bytes": 1048576,
                "timeout_per_file_seconds": 300
            }
        }
    
    def _load_config(self, config_path: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                file_config = json.load(f)
                # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–ª—é—á–∏
                for key, value in file_config.items():
                    if key in self.config:
                        if isinstance(value, dict) and isinstance(self.config[key], dict):
                            self.config[key].update(value)
                        else:
                            self.config[key] = value
            logger.info(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {config_path}")
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.")
    
    def get(self, *keys):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ –∫–ª—é—á–∞–º"""
        value = self.config
        for key in keys:
            if isinstance(value, dict) and key in value:
                value = value[key]
            else:
                return None
        return value

class ResourceMonitor:
    """–ö–ª–∞—Å—Å –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤"""
    
    def __init__(self, memory_limit_mb: int = 2048):
        self.memory_limit_mb = memory_limit_mb
        self.start_time = time.time()
    
    def check_memory_usage(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
        if not HAS_PSUTIL:
            return True
        
        try:
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            
            if memory_mb > self.memory_limit_mb:
                logger.warning(f"–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –ø–∞–º—è—Ç–∏: {memory_mb:.1f}MB > {self.memory_limit_mb}MB")
                return False
            
            return True
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø–∞–º—è—Ç–∏: {e}")
            return True
    
    def get_elapsed_time(self) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""
        return time.time() - self.start_time
    
    def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏"""
        gc.collect()
        if HAS_TRANSFORMERS and torch.cuda.is_available():
            torch.cuda.empty_cache()

def load_glossary(glossary_path: Optional[str]) -> Dict[str, str]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –≥–ª–æ—Å—Å–∞—Ä–∏—è –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤"""
    if not glossary_path or not os.path.exists(glossary_path):
        return {}
    try:
        with open(glossary_path, 'r', encoding='utf-8') as f:
            glossary = json.load(f)
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω –≥–ª–æ—Å—Å–∞—Ä–∏–π —Å {len(glossary)} —Ç–µ—Ä–º–∏–Ω–∞–º–∏")
            return glossary
    except (json.JSONDecodeError, IOError) as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≥–ª–æ—Å—Å–∞—Ä–∏—è: {e}")
        return {}

def apply_glossary(text: str, glossary: Dict[str, str]) -> str:
    """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≥–ª–æ—Å—Å–∞—Ä–∏—è –∫ —Ç–µ–∫—Å—Ç—É"""
    if not glossary:
        return text
    
    for term, translation in glossary.items():
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
        pattern = r'\b' + re.escape(term) + r'\b'
        text = re.sub(pattern, translation, text, flags=re.IGNORECASE)
    
    return text

def preserve_markdown_elements(text: str) -> Tuple[str, Dict[str, str]]:
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Ä–∞–∑–º–µ—Ç–∫–∏ –æ—Ç –ø–µ—Ä–µ–≤–æ–¥–∞"""
    placeholders = {}
    counter = 0
    
    # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤ Markdown
    patterns = [
        (r'```[\s\S]*?```', 'CODE_BLOCK'),  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –±–ª–æ–∫–∏ –∫–æ–¥–∞
        (r'`[^`\n]+`', 'INLINE_CODE'),
        (r'\[([^\]]*)\]\(([^)]*)\)', 'LINK'),
        (r'<!--[\s\S]*?-->', 'COMMENT'),
        (r'<[^>]+>', 'HTML_TAG'),
        (r'!\[([^\]]*)\]\(([^)]*)\)', 'IMAGE'),  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        (r'\{%[\s\S]*?%\}', 'TEMPLATE'),
        (r'\$\$[\s\S]*?\$\$', 'MATH_BLOCK'),
        (r'\$[^$\n]+\$', 'MATH_INLINE'),
        (r'#+\s+[^\n]+', 'HEADER'),  # –ó–∞–≥–æ–ª–æ–≤–∫–∏
        (r'\|[^\n]+\|', 'TABLE_ROW'),  # –°—Ç—Ä–æ–∫–∏ —Ç–∞–±–ª–∏—Ü
    ]
    
    for pattern, element_type in patterns:
        def replace_func(match):
            nonlocal counter
            placeholder = f'__PLACEHOLDER_{element_type}_{counter}__'
            placeholders[placeholder] = match.group(0)
            counter += 1
            return placeholder
        
        text = re.sub(pattern, replace_func, text)
    
    return text, placeholders

def restore_preserved_elements(text: str, placeholders: Dict[str, str]) -> str:
    """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
    for placeholder, original in placeholders.items():
        text = text.replace(placeholder, original)
    return text

def translate_text_simple(text: str, glossary: Optional[Dict[str, str]] = None) -> str:
    """–ü—Ä–æ—Å—Ç–æ–π –ø–µ—Ä–µ–≤–æ–¥ –±–µ–∑ AI –º–æ–¥–µ–ª–∏ (–∑–∞–≥–ª—É—à–∫–∞)"""
    if not text.strip():
        return text
    
    if glossary:
        text = apply_glossary(text, glossary)
    
    logger.debug("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ—Å—Ç–æ–π —Ä–µ–∂–∏–º –ø–µ—Ä–µ–≤–æ–¥–∞ (–∑–∞–≥–ª—É—à–∫–∞)")
    return text

def translate_text_ai(text: str, model, tokenizer, glossary: Optional[Dict[str, str]] = None, 
                     max_length: int = 512, timeout: int = 300) -> str:
    """–ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º AI –º–æ–¥–µ–ª–∏ —Å —Ç–∞–π–º–∞—É—Ç–æ–º"""
    if not text.strip():
        return text
    
    start_time = time.time()
    
    try:
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≥–ª–æ—Å—Å–∞—Ä–∏–π –¥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Ä–∞–∑–º–µ—Ç–∫–∏
        if glossary:
            text = apply_glossary(text, glossary)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Ä–∞–∑–º–µ—Ç–∫–∏
        text_to_translate, placeholders = preserve_markdown_elements(text)
        
        if not text_to_translate.strip():
            return text
        
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏
        chunks = []
        sentences = re.split(r'(?<=[.!?])\s+', text_to_translate)
        current_chunk = ""
        
        for sentence in sentences:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∞–π–º–∞—É—Ç–∞
            if time.time() - start_time > timeout:
                logger.warning("–ü—Ä–µ–≤—ã—à–µ–Ω —Ç–∞–π–º–∞—É—Ç –ø–µ—Ä–µ–≤–æ–¥–∞")
                break
            
            if len(current_chunk + sentence) > max_length:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = sentence
                else:
                    # –†–∞–∑–±–∏–≤–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
                    chunks.append(sentence[:max_length])
            else:
                current_chunk += " " + sentence if current_chunk else sentence
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å—Ç–µ–π
        if len(chunks) > 50:  # –ú–∞–∫—Å–∏–º—É–º 50 —á–∞—Å—Ç–µ–π
            logger.warning(f"–¢–µ–∫—Å—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(chunks)} —á–∞—Å—Ç–µ–π, –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 50")
            chunks = chunks[:50]
        
        translated_chunks = []
        
        for i, chunk in enumerate(chunks):
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∞–π–º–∞—É—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —á–∞—Å—Ç–∏
            if time.time() - start_time > timeout:
                logger.warning("–ü—Ä–µ–≤—ã—à–µ–Ω —Ç–∞–π–º–∞—É—Ç –ø–µ—Ä–µ–≤–æ–¥–∞")
                break
            
            if not chunk.strip():
                translated_chunks.append(chunk)
                continue
            
            try:
                inputs = tokenizer(
                    chunk, 
                    return_tensors="pt", 
                    padding=True, 
                    truncation=True,
                    max_length=max_length
                ).to(model.device)
                
                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=max_length,
                        num_beams=3,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                        early_stopping=True,
                        temperature=0.7,
                        do_sample=False
                    )
                
                translated_chunk = tokenizer.decode(outputs[0], skip_special_tokens=True)
                translated_chunks.append(translated_chunk)
                
                # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 10 —á–∞—Å—Ç–µ–π
                if (i + 1) % 10 == 0:
                    logger.debug(f"–ü–µ—Ä–µ–≤–µ–¥–µ–Ω–æ {i + 1}/{len(chunks)} —á–∞—Å—Ç–µ–π")
                
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ —á–∞—Å—Ç–∏ {i + 1}: {e}")
                translated_chunks.append(chunk)  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –ø—Ä–∏ –æ—à–∏–±–∫–µ
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã–µ —á–∞—Å—Ç–∏
        translated_text = " ".join(translated_chunks)
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Ä–∞–∑–º–µ—Ç–∫–∏
        translated_text = restore_preserved_elements(translated_text, placeholders)
        
        return translated_text
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ AI –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
        return text

def process_markdown_file(src_path: str, dest_path: str, model, tokenizer, 
                         glossary: Dict[str, str], config: TranslationConfig,
                         monitor: ResourceMonitor) -> bool:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ Markdown —Ñ–∞–π–ª–∞ —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º —Ä–µ—Å—É—Ä—Å–æ–≤"""
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞
        file_size = os.path.getsize(src_path)
        max_size = config.get('limits', 'max_file_size_bytes') or 1048576
        
        if file_size > max_size:
            logger.warning(f"–§–∞–π–ª {src_path} –ø—Ä–µ–≤—ã—à–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä ({file_size} > {max_size} –±–∞–π—Ç)")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏
        if not monitor.check_memory_usage():
            logger.warning(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª {src_path} –∏–∑-–∑–∞ –Ω–µ—Ö–≤–∞—Ç–∫–∏ –ø–∞–º—è—Ç–∏")
            return False
        
        # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        with open(src_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è
        os.makedirs(os.path.dirname(dest_path), exist_ok=True)
        
        # –ü–µ—Ä–µ–≤–æ–¥
        timeout = config.get('limits', 'timeout_per_file_seconds') or 300
        
        if model and tokenizer:
            max_tokens = config.get('limits', 'max_tokens_per_input') or 512
            translated_content = translate_text_ai(content, model, tokenizer, glossary, max_tokens, timeout)
        else:
            translated_content = translate_text_simple(content, glossary)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        with open(dest_path, 'w', encoding='utf-8') as f:
            f.write(translated_content)
        
        logger.info(f"‚úì –ü–µ—Ä–µ–≤–µ–¥–µ–Ω: {Path(src_path).name} ({file_size} –±–∞–π—Ç)")
        return True
        
    except Exception as e:
        logger.error(f"‚úó –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {src_path}: {e}")
        return False

def process_image_file(src_path: str, dest_path: str, model=None, tokenizer=None, 
                      glossary: Optional[Dict[str, str]] = None, config: Optional[TranslationConfig] = None) -> bool:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    try:
        os.makedirs(os.path.dirname(dest_path), exist_ok=True)
        
        # –ï—Å–ª–∏ OCR –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ—Å—Ç–æ –∫–æ–ø–∏—Ä—É–µ–º
        if not HAS_OCR:
            copyfile(src_path, dest_path)
            logger.info(f"‚úì –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {Path(src_path).name}")
            return True
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if config:
            max_image_size = config.get('limits', 'max_image_size_bytes') or 10485760
            file_size = os.path.getsize(src_path)
            if file_size > max_image_size:
                logger.warning(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {src_path} —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–µ ({file_size} > {max_image_size} –±–∞–π—Ç)")
                copyfile(src_path, dest_path)
                return True
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        original_text, image = extract_text_from_image(src_path)
        
        if not original_text or not image:
            copyfile(src_path, dest_path)
            logger.info(f"‚úì –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω): {Path(src_path).name}")
            return True
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞
        max_text_length = config.get('limits', 'max_text_length_on_image') if config else 5000
        if max_text_length and len(original_text) > max_text_length:
            logger.info(f"–¢–µ–∫—Å—Ç –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π ({len(original_text)} > {max_text_length}), –∫–æ–ø–∏—Ä—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª")
            copyfile(src_path, dest_path)
            return True
        
        # –ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞
        if model and tokenizer:
            translated_text = translate_text_ai(original_text, model, tokenizer, glossary)
        else:
            translated_text = translate_text_simple(original_text, glossary)
        
        # –ù–∞–ª–æ–∂–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–∞
        new_image = overlay_text_on_image(image, translated_text)
        
        if new_image:
            new_image.save(dest_path, quality=95, optimize=True)
            logger.info(f"‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å —Ç–µ–∫—Å—Ç–æ–º: {Path(src_path).name}")
        else:
            copyfile(src_path, dest_path)
            logger.info(f"‚úì –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {Path(src_path).name}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚úó –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {src_path}: {e}")
        return False

def extract_text_from_image(image_path: str, lang: str = 'rus') -> Tuple[str, Optional['Image.Image']]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é Tesseract OCR"""
    if not HAS_OCR:
        logger.debug("OCR –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç")
        return "", None
    
    try:
        image = Image.open(image_path)
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
        config = '--oem 3 --psm 6 -c tessedit_char_whitelist=–ê–ë–í–ì–î–ï–Å–ñ–ó–ò–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–ß–®–©–™–´–¨–≠–Æ–Ø–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—èABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789.,!?;:()[]{}"\'-+= '
        text = pytesseract.image_to_string(image, lang=lang, config=config)
        return text.strip(), image
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ {image_path}: {e}")
        return "", None

def overlay_text_on_image(image: 'Image.Image', translated_text: str) -> Optional['Image.Image']:
    """–ù–∞–ª–æ–∂–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"""
    if not HAS_OCR:
        return image
    
    try:
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        if len(translated_text) > 500:
            translated_text = translated_text[:497] + "..."
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å OpenCV
        image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        height, width = image_cv.shape[:2]
        
        # –°–æ–∑–¥–∞–µ–º –æ–±–ª–∞—Å—Ç—å –¥–ª—è —Ç–µ–∫—Å—Ç–∞
        text_height = 100
        new_height = height + text_height
        new_image = np.ones((new_height, width, 3), dtype=np.uint8) * 255
        
        # –í—Å—Ç–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        new_image[:height, :] = image_cv
        
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Å—Ç—Ä–æ–∫–∏
        words = translated_text.split()
        lines = []
        current_line = ""
        
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.6
        thickness = 1
        max_width = width - 20
        
        for word in words:
            test_line = current_line + " " + word if current_line else word
            text_size = cv2.getTextSize(test_line, font, font_scale, thickness)[0]
            
            if text_size[0] <= max_width:
                current_line = test_line
            else:
                if current_line:
                    lines.append(current_line)
                    current_line = word
                else:
                    # –ï—Å–ª–∏ –¥–∞–∂–µ –æ–¥–Ω–æ —Å–ª–æ–≤–æ –Ω–µ –ø–æ–º–µ—â–∞–µ—Ç—Å—è, —Ä–∞–∑–±–∏–≤–∞–µ–º –µ–≥–æ
                    lines.append(word)
        
        if current_line:
            lines.append(current_line)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫
        lines = lines[:3]
        
        # –ù–∞–∫–ª–∞–¥—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç
        line_height = 20
        start_y = height + 25
        
        for i, line in enumerate(lines):
            y = start_y + i * line_height
            cv2.putText(
                new_image,
                line,
                (10, y),
                font,
                font_scale,
                (0, 0, 0),
                thickness,
                cv2.LINE_AA
            )
        
        return Image.fromarray(cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB))
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –Ω–∞–ª–æ–∂–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {e}")
        return image

def should_process_file(src_path: str, dest_path: str, force: bool = False) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞, –Ω—É–∂–Ω–æ –ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ñ–∞–π–ª"""
    if force:
        return True
    
    if not os.path.exists(dest_path):
        return True
    
    try:
        return os.path.getmtime(src_path) > os.path.getmtime(dest_path)
    except OSError:
        return True

def load_translation_model():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–≤–æ–¥–∞"""
    if not HAS_TRANSFORMERS:
        logger.warning("Transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, AI –ø–µ—Ä–µ–≤–æ–¥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        return None, None
    
    model_name = "Helsinki-NLP/opus-mt-ru-en"
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model = model.to(device)
        
        logger.info(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {device.upper()}")
        return model, tokenizer
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return None, None

def collect_files_to_process(source_dir: str, max_files: int, force: bool = False, 
                           target_dir: str = None) -> List[Tuple[str, str]]:
    """–°–±–æ—Ä —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞"""
    files_to_process = []
    supported_extensions = {'.md', '.png', '.jpg', '.jpeg', '.gif', '.bmp'}
    
    # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –æ–±—Ö–æ–¥ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    for root, dirs, files in os.walk(source_dir):
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∫—Ä—ã—Ç—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        dirs[:] = [d for d in dirs if not d.startswith('.')]
        
        for file in files:
            if file.startswith('.') or file in ['Thumbs.db', '.DS_Store']:
                continue
            
            file_ext = Path(file).suffix.lower()
            if file_ext not in supported_extensions:
                continue
            
            src_path = os.path.join(root, file)
            rel_path = os.path.relpath(src_path, source_dir)
            dest_path = os.path.join(target_dir, rel_path) if target_dir else None
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ñ–∞–π–ª
            if dest_path and should_process_file(src_path, dest_path, force):
                files_to_process.append((src_path, dest_path))
                
                # –ü—Ä–µ—Ä—ã–≤–∞–µ–º, –µ—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏–º–∏—Ç–∞
                if len(files_to_process) >= max_files:
                    logger.info(f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —Ñ–∞–π–ª–æ–≤: {max_files}")
                    break
        
        if len(files_to_process) >= max_files:
            break
    
    return files_to_process

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description='Translate documentation from Russian to English')
    parser.add_argument('--source_dir', default='./ru', help='Source directory with Russian content')
    parser.add_argument('--target_dir', default='./en', help='Target directory for English content')
    parser.add_argument('--glossary', help='Path to glossary JSON file')
    parser.add_argument('--config', help='Path to configuration JSON file')
    parser.add_argument('--force', action='store_true', help='Force reprocessing of all files')
    parser.add_argument('--no-ai', action='store_true', help='Disable AI translation')
    parser.add_argument('--verbose', '-v', action='store_true', help='Verbose output')
    parser.add_argument('--max-files', type=int, help='Maximum files to process (overrides config)')
    parser.add_argument('--max-chars', type=int, help='Maximum characters per translation chunk')
    parser.add_argument('--max-tokens', type=int, help='Maximum tokens per model input')
    parser.add_argument('--max-file-size', type=int, help='Maximum file size in bytes')
    
    args = parser.parse_args()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    if args.verbose:
        logger.setLevel(logging.DEBUG)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    if not os.path.exists(args.source_dir):
        logger.error(f"–ò—Å—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {args.source_dir}")
        return False
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    config = TranslationConfig(args.config)
    
    # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    if args.max_files is not None:
        if not config.config.get('limits'):
            config.config['limits'] = {}
        config.config['limits']['max_files_per_run'] = args.max_files
    
    if args.max_chars is not None:
        config.config['limits']['max_chars_per_chunk'] = args.max_chars
    
    if args.max_tokens is not None:
        config.config['limits']['max_tokens_per_input'] = args.max_tokens
    
    if args.max_file_size is not None:
        config.config['limits']['max_file_size_bytes'] = args.max_file_size
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –ª–∏–º–∏—Ç–æ–≤
    max_files = config.get('limits', 'max_files_per_run') or 20
    memory_limit = config.get('limits', 'max_memory_usage_mb') or 2048
    
    logger.info(f"–ú–∞–∫—Å–∏–º—É–º —Ñ–∞–π–ª–æ–≤ –∑–∞ –ø—Ä–æ–≥–æ–Ω: {max_files}")
    logger.info(f"–õ–∏–º–∏—Ç –ø–∞–º—è—Ç–∏: {memory_limit}MB")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ä–µ—Å—É—Ä—Å–æ–≤
    monitor = ResourceMonitor(memory_limit)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –≥–ª–æ—Å—Å–∞—Ä–∏—è
    glossary = load_glossary(args.glossary)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–≤–æ–¥–∞
    model, tokenizer = None, None
    if not args.no_ai:
        model, tokenizer = load_translation_model()
        if model and tokenizer:
            max_tokens = config.get('limits', 'max_tokens_per_input') or 512
            max_chars = config.get('limits', 'max_chars_per_chunk') or 10000
            logger.info(f"AI –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞—Å—Ç—Ä–æ–µ–Ω: –º–∞–∫—Å. {max_chars} —Å–∏–º–≤–æ–ª–æ–≤, {max_tokens} —Ç–æ–∫–µ–Ω–æ–≤")
    
    # –°–±–æ—Ä —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    files_to_process = collect_files_to_process(
        args.source_dir, 
        max_files, 
        args.force, 
        args.target_dir
    )
    
    if not files_to_process:
        logger.info("–ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return True
    
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(files_to_process)} —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    
    # –°—á–µ—Ç—á–∏–∫–∏
    processed_count = 0
    error_count = 0
    start_time = time.time()
    
    try:
        for i, (src_path, dest_path) in enumerate(files_to_process):
            logger.debug(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ {i + 1}/{len(files_to_process)}: {Path(src_path).name}")
            
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            if i > 0 and i % 5 == 0:
                monitor.cleanup()
                if not monitor.check_memory_usage():
                    logger.warning("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É")
                    break
            
            success = False
            file_ext = Path(src_path).suffix.lower()
            
            try:
                if file_ext == '.md':
                    success = process_markdown_file(src_path, dest_path, model, tokenizer, glossary, config, monitor)
                elif file_ext in {'.png', '.jpg', '.jpeg', '.gif', '.bmp'}:
                    success = process_image_file(src_path, dest_path, model, tokenizer, glossary, config)
                else:
                    # –ö–æ–ø–∏—Ä—É–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã
                    os.makedirs(os.path.dirname(dest_path), exist_ok=True)
                    copyfile(src_path, dest_path)
                    logger.info(f"‚úì –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω: {Path(src_path).name}")
                    success = True
                    
            except KeyboardInterrupt:
                logger.info("–ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                break
            except Exception as e:
                logger.error(f"‚úó –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {src_path}: {e}")
                success = False
            
            if success:
                processed_count += 1
            else:
                error_count += 1
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 5 —Ñ–∞–π–ª–æ–≤
            if (i + 1) % 5 == 0:
                elapsed_time = monitor.get_elapsed_time()
                logger.info(f"–ü—Ä–æ–≥—Ä–µ—Å—Å: {i + 1}/{len(files_to_process)}, –≤—Ä–µ–º—è: {elapsed_time:.1f}—Å")
    
    finally:
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        monitor.cleanup()
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    elapsed_time = monitor.get_elapsed_time()
    logger.info("=" * 50)
    logger.info("–ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    logger.info(f"‚úì –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_count}")
    logger.info(f"‚úó –û—à–∏–±–æ–∫: {error_count}")
    logger.info(f"‚è± –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {elapsed_time:.1f} —Å–µ–∫—É–Ω–¥")
    logger.info(f"üìä –õ–∏–º–∏—Ç —Ñ–∞–π–ª–æ–≤: {max_files}")
    logger.info(f"üîß –†–µ–∂–∏–º AI: {'–≤–∫–ª—é—á–µ–Ω' if model else '–≤—ã–∫–ª—é—á–µ–Ω'}")
    
    if HAS_PSUTIL:
        try:
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            logger.info(f"üíæ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {memory_mb:.1f}MB")
        except:
            pass
    
    logger.info("=" * 50)
    
    return error_count == 0

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        logger.info("–ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1)
